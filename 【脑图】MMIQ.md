- 论文概述
  - 题目：MM-IQ：多模态模型中人类抽象与推理能力的基准
  - 主要贡献：
    - 提出一个综合评估框架MM-IQ，涵盖8种不同的推理模式，共2710项精心挑选的问题。
    - 系统性地评估了开源及闭源多模态模型的表现，并揭示了现有模型在抽象推理方面的局限性。

- 引言
  - 多模态模型发展迅速，引发了关于其能否具备类似人类的抽象与推理能力的讨论。
  - 传统评估方法受限于语言或特定知识，智商测试提供了一种评估人类核心认知能力的方法。
  - 目标：建立一个多模态模型智能评估框架，不依赖于特定的语言、知识背景，专注于抽象推理能力。

- 相关工作
  - 分类现有的AVR（抽象视觉推理）任务，包括输入形状、问题配置、推理模式等维度。
  - 比较MM-IQ与其他AVR基准，如RAVEN、G-set等，强调MM-IQ的全面性和多样性。

- 构建MM-IQ
  - 数据收集：主要来源于中国国家公务员考试题目，经过筛选和分类，确保问题的质量和多样性。
  - 推理模式扩展：从现有的AVR基准中汲取灵感，细化为8种推理模式，以全面评估模型的能力。

- 实验设置与结果
  - 实验目的：评估当前多模态模型在抽象推理任务上的表现。
  - 方法：使用零样本提示，比较不同规模和类型的模型。
  - 结果：所有模型的表现远低于人类水平，最大的差距出现在逻辑操作和数学推理领域。

- 失败分析
  - 错误类型：
    - 不正确的推理方式
    - 视觉理解错误
    - 最终答案错误
  - 分析表明，模型在识别抽象规则、复杂图形理解和生成结构化回答方面存在显著困难。

- 结论
  - MM-IQ填补了多模态模型智能评估的空白，指出当前模型在抽象推理能力方面存在的巨大鸿沟。
  - 指导未来研究方向：改进模型的结构性输出、提高对抽象规律的理解、增强复杂的视觉理解能力。

